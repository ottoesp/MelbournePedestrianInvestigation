{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd700607",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-15T09:48:53.907518300Z",
     "start_time": "2026-01-15T09:48:53.893520300Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.axes\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to Python path\n",
    "project_root = Path().resolve().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "import src.config as config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cec119",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-15T09:48:53.946336800Z",
     "start_time": "2026-01-15T09:48:53.910574100Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import src.loadProcessed as loadp\n",
    "\n",
    "counts: pd.DataFrame = loadp.load_selected_count()\n",
    "locations: pd.DataFrame = loadp.load_processed_locations()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba34f8c",
   "metadata": {},
   "source": [
    "## Compute Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd15606",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-15T09:48:53.989927100Z",
     "start_time": "2026-01-15T09:48:53.981014100Z"
    }
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import patsy.highlevel as phl\n",
    "import math\n",
    "\n",
    "# We want to log transform since counts have a power relation\n",
    "log_data = counts.copy()\n",
    "log_data['daily_count'] = log_data['daily_count'].apply(np.log)\n",
    "log_data['is_weekend'] = log_data['day'].isin(['Saturday', 'Sunday'])\n",
    "\n",
    "model_3 = smf.mixedlm(\n",
    "    'daily_count ~ day*year',     # Defines the response and fixed effects\n",
    "    log_data,                       \n",
    "    groups=log_data['sensor_id'],  # Defines how to cluster the data\n",
    "    re_formula='~ is_weekend*year'            # Random effect that differs across the clusters\n",
    ")\n",
    "result_3 = model_3.fit(reml=True)\n",
    "\n",
    "print(result_3.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399e7fe6",
   "metadata": {},
   "source": [
    "## Extract Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d19fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NumPy array maybe overkill but it's nice to have set functions that maintain order\n",
    "days = np.array(['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'])\n",
    "day_levels = np.setdiff1d(days, ['Friday'], assume_unique=True)\n",
    "\n",
    "residual_variance = result_3.scale\n",
    "\n",
    "# --- Fixed Covariance Matrix ---\n",
    "fe_label_order = (\n",
    "    ['Intercept', 'year[T.2025]']\n",
    "    + [f'day[T.{d}]' for d in day_levels]\n",
    "    + [f'day[T.{d}]:year[T.2025]' for d in day_levels]\n",
    ")\n",
    "fixed_cov_matrix = result_3.cov_params().loc[fe_label_order, fe_label_order]\n",
    "\n",
    "fe_rename_map = {'Intercept': 'intercept', 'year[T.2025]': 'year'}\n",
    "fe_rename_map.update({f'day[T.{d}]': f'day_{d.lower()}' for d in day_levels})\n",
    "fe_rename_map.update({f'day[T.{d}]:year[T.2025]': f'interaction_{d.lower()}' for d in day_levels})\n",
    "\n",
    "fixed_cov_matrix = fixed_cov_matrix.rename(index=fe_rename_map, columns=fe_rename_map)\n",
    "\n",
    "# -- Fixed Params Vector (Beta) --\n",
    "fixed_params = result_3.params.loc[fe_label_order].rename(index=fe_rename_map)\n",
    "\n",
    "\n",
    "# --- Random Covariance Matrix ---\n",
    "random_cov_matrix = result_3.cov_re.iloc[:4, :4]\n",
    "\n",
    "# Reorder and rename the covariance matrix\n",
    "re_label_order = ['Group', 'year[T.2025]', 'is_weekend[T.True]', 'is_weekend[T.True]:year[T.2025]']\n",
    "re_rename_labels = ['sensor', 'year', 'weekend', 'interaction']\n",
    "\n",
    "re_rename_map = dict(zip(re_label_order, re_rename_labels))\n",
    "\n",
    "random_cov_matrix = random_cov_matrix.loc[re_label_order, re_label_order]\n",
    "\n",
    "# Rename rows and columns\n",
    "random_cov_matrix = random_cov_matrix.rename(index=re_rename_map, columns=re_rename_map)\n",
    "\n",
    "# --- Random Params Vector (b) ---\n",
    "random_params = (\n",
    "    pd.DataFrame.from_dict(result_3.random_effects, orient='index')\n",
    "    .reindex(columns=re_label_order)\n",
    "    .rename(columns=re_rename_map)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa1168e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Fixed Effects\n",
    "log_fixed_year = result_3.params['year[T.2025]']  # beta_1\n",
    "log_fixed_intercept = result_3.params['Intercept']  # beta_0\n",
    "\n",
    "log_fixed_day_effects = {d: result_3.params[f'day[T.{d}]'] for d in day_levels}\n",
    "\n",
    "# Extract Random Effects\n",
    "log_random_effects = pd.DataFrame.from_dict(\n",
    "    result_3.random_effects,\n",
    "    orient='index'\n",
    ").rename(\n",
    "    columns={\n",
    "        'Group': 'b0_intercept',\n",
    "        'year[T.2025]': 'b1_year',\n",
    "        'is_weekend[T.True]': 'b2_weekend',\n",
    "        'is_weekend[T.True]:year[T.2025]': 'b3_interaction'\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0714fb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_design_vector(intercept: bool, year: bool, weekend: bool):\n",
    "    return np.array(\n",
    "        [int(intercept), int(year), int(weekend), int(year and weekend)]\n",
    "    )\n",
    "\n",
    "def fixed_design_vector(intercept: bool, year: bool, day: str):\n",
    "    # Create a fixed effects design vector\n",
    "    day_vector = np.array([0] * len(day_levels))\n",
    "\n",
    "    # Leave as all zeros if day is Friday\n",
    "    if day != 'Friday':\n",
    "        # Find the corresponding index to the specified day\n",
    "        day_vector[np.where(day_levels == day)[0][0]] = 1\n",
    "\n",
    "    return np.concatenate(([int(intercept), int(year)], day_vector, day_vector * int(year)))\n",
    "\n",
    "def random_effect_sensor_cov(sensor_id):\n",
    "    cov_mat = result_3.random_effects_cov[sensor_id]\n",
    "\n",
    "    cov_mat = cov_mat.loc[re_label_order, re_label_order]\n",
    "\n",
    "    # Rename rows and columns\n",
    "    cov_mat = cov_mat.rename(index=re_rename_map, columns=re_rename_map)\n",
    "    return cov_mat\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc21591",
   "metadata": {},
   "source": [
    "## Calculating Population Wide Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fa85f3",
   "metadata": {},
   "source": [
    "Where $\\boldsymbol{x}_j^T$, $\\boldsymbol{z}_j^T$ are rows corresponding to the $j$ in their respective design matrices, $\\boldsymbol{b}_j \\sim N(\\bold 0, G)$, $\\varepsilon_j \\sim N(\\bold 0, \\sigma^2)$\n",
    "$$\\log y_{j} = \\boldsymbol{x}_j^T \\boldsymbol{\\beta} + \\boldsymbol{z}_j^T \\boldsymbol{b}_j + \\varepsilon_j$$\n",
    "$$\\log E[y_j] = \\boldsymbol{x}_j^T \\boldsymbol{\\beta} + \\frac 1 2 (\\boldsymbol{z}_j^T G \\boldsymbol{z}_j + \\sigma ^ 2)$$\n",
    "$$\\text{Var}\\ \\log E[y_j] \\approx \\text{Var}\\ \\boldsymbol{x}_j^T \\hat{\\boldsymbol{\\beta}} = \\boldsymbol{x}_j^T \\text{Var}(\\hat{\\boldsymbol{\\beta}}) \\boldsymbol{x}_j$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c3f4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "population_wide_stats = pd.DataFrame(\n",
    "    index = ['estimate', 'lower', 'upper']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8979d30c",
   "metadata": {},
   "source": [
    "### Estimates of population counts for each day across years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5af341f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for day in days:\n",
    "    for year in ['2019', '2025']:\n",
    "        is_weekend = day in ['Saturday', 'Sunday']\n",
    "        t_t = fixed_design_vector(True, year == '2025', day)\n",
    "        z_t = random_design_vector(True, year == '2025', is_weekend)\n",
    "\n",
    "        # Calculate Estimate\n",
    "        log_estimate = (\n",
    "            t_t @ fixed_params + (z_t @ random_cov_matrix @ z_t.transpose() + residual_variance)/2\n",
    "        )\n",
    "        population_wide_stats.loc['estimate', f'{day}_expected_{year}'] = np.exp(log_estimate)\n",
    "        \n",
    "        # Calculate Confidence interval\n",
    "        std_error = np.sqrt(t_t @ fixed_cov_matrix @ t_t.transpose())\n",
    "        \n",
    "        population_wide_stats.loc['lower', f'{day}_expected_{year}'] = np.exp(log_estimate - 1.96 * std_error)\n",
    "        population_wide_stats.loc['upper', f'{day}_expected_{year}'] = np.exp(log_estimate + 1.96 * std_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a411f451",
   "metadata": {},
   "outputs": [],
   "source": [
    "for day in days:\n",
    "    x_t25 = fixed_design_vector(True, True, day)\n",
    "    x_t19 = fixed_design_vector(True, False, day)\n",
    "\n",
    "    z_t25 = random_design_vector(True, True, day in ['Saturday', 'Sunday'])\n",
    "    z_t19 = random_design_vector(True, False, day in ['Saturday', 'Sunday'])\n",
    "\n",
    "    log_ratio_estimate = (x_t25 - x_t19) @ fixed_params + (z_t25 @ random_cov_matrix @ z_t25.transpose() - z_t19 @ random_cov_matrix @ z_t19.transpose())/2 \n",
    "\n",
    "    std_error = np.sqrt((x_t25 - x_t19) @ fixed_cov_matrix @ (x_t25 - x_t19).transpose())\n",
    "\n",
    "    population_wide_stats.loc['estimate', f'{day}_percentage_change'] = (np.exp(log_ratio_estimate) - 1) * 100\n",
    "    population_wide_stats.loc['lower', f'{day}_percentage_change'] = (np.exp(log_ratio_estimate - 1.96 * std_error) - 1) * 100\n",
    "    population_wide_stats.loc['upper', f'{day}_percentage_change'] = (np.exp(log_ratio_estimate + 1.96 * std_error) - 1) * 100\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e613a31",
   "metadata": {},
   "source": [
    "### Estimates of average population counts across years\n",
    "Possible extenstion of this would be calculating the confidence intervals, however since it is a non-linear transforrmation of our estimators I would need to research methods for this. From a brief look I can see the delta method and bootstrapping may be potential avenues for improvement but for the scope of this project I will leave this as a point estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8604c349",
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in ['2019', '2025']:\n",
    "    columns = [f'{day}_expected_{year}' for day in days]\n",
    "\n",
    "    weekly_sum = population_wide_stats.loc['estimate', columns].sum() # type: ignore\n",
    "    population_wide_stats.loc['estimate', f'mean_expected_{year}'] = weekly_sum / 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35799c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "population_wide_stats.to_parquet(config.PROCESSED_DATA_DIR / 'population_stats.parquet')\n",
    "population_wide_stats.to_csv(config.PROCESSED_DATA_DIR / 'population_stats.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccafd91c",
   "metadata": {},
   "source": [
    "## Location Specific Estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72994608",
   "metadata": {},
   "source": [
    "Where $\\boldsymbol{x}_j^T$, $\\boldsymbol{z}_j^T$ are rows corresponding to the $j$ in their respective design matrices, $\\boldsymbol{b}_j \\sim N(\\bold 0, G)$, $\\varepsilon_j \\sim N(\\bold 0, \\sigma^2)$\n",
    "$$\\log y_{j} = \\boldsymbol{x}_j^T \\boldsymbol{\\beta} + \\boldsymbol{z}_j^T \\boldsymbol{b}_j + \\varepsilon_j$$\n",
    "$$\\log E[y_j | \\boldsymbol{b}_j] = \\boldsymbol{x}_j^T \\boldsymbol{\\beta} + \\boldsymbol{z}_j^T \\boldsymbol{b}_j + \\frac {\\sigma ^ 2} 2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7d70fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "location_specific_stats = pd.DataFrame(\n",
    "    index = random_params.index\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ab8144",
   "metadata": {},
   "source": [
    "### Absolute Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59628b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sensor_id, sensor_b in random_params.iterrows():\n",
    "    for year in ['2019', '2025']:\n",
    "        for is_weekend in [False, True]:       \n",
    "            if is_weekend:     \n",
    "                average_over_days = np.array(['Saturday', 'Sunday'])\n",
    "            else:\n",
    "                average_over_days = np.setdiff1d(days, ['Saturday', 'Sunday'])\n",
    "\n",
    "            z_t = random_design_vector(True, year == '2025', is_weekend)\n",
    "\n",
    "            X_t = np.array([fixed_design_vector(True, year == '2025', d) for d in average_over_days])\n",
    "\n",
    "            count_estimate = (\n",
    "                np.exp(z_t @ sensor_b + residual_variance/2) * 1/len(average_over_days) * (np.exp(X_t @ fixed_params)).sum()\n",
    "            )\n",
    "\n",
    "            location_specific_stats.loc[sensor_id, f'{'wknd' if is_weekend else 'wkdy'}_count_{year}_estimate'] = count_estimate\n",
    "\n",
    "for year in ['2019', '2025']:\n",
    "    location_specific_stats[f'count_{year}_estimate'] = 2/7 * location_specific_stats[f'wknd_count_{year}_estimate'] + 5/7 * location_specific_stats[f'wkdy_count_{year}_estimate'] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14320a7f",
   "metadata": {},
   "source": [
    "## Fractional Change\n",
    "Since the weekend random effect requires a sum over the day fixed effects, the ratio of these sums does not cancel as nicely as usual under a log transformation. For this reason we are taking the ratio of absolute counts despite the added variability this induces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24d0861",
   "metadata": {},
   "outputs": [],
   "source": [
    "location_specific_stats['percent_year_change'] = (location_specific_stats['count_2025_estimate'] / location_specific_stats['count_2019_estimate'] - 1) * 100\n",
    "location_specific_stats['wknd_percent_year_change'] = (location_specific_stats['wknd_count_2025_estimate'] / location_specific_stats['wknd_count_2019_estimate'] - 1) * 100\n",
    "location_specific_stats['wkdy_percent_year_change'] = (location_specific_stats['wkdy_count_2025_estimate'] / location_specific_stats['wkdy_count_2019_estimate'] - 1) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c15bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "location_specific_stats.to_parquet(config.PROCESSED_DATA_DIR / 'location_specific_stats.parquet')\n",
    "location_specific_stats.to_csv(config.PROCESSED_DATA_DIR / 'location_specific_stats.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
